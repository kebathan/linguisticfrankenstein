{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set('‚ñÅ\"._abcdefghijklmnopqrstuvwxyz')\n",
    "with (\n",
    "    open(\"data\\\\trial2\\\\train_inputs.txt\", \"r\") as src,\n",
    "    open(\"data\\\\trial2\\\\test_inputs.txt\", \"r\") as tgt,\n",
    "):\n",
    "    source_data = src.read()\n",
    "    target_data = tgt.read()\n",
    "\n",
    "missing_chars = set(source_data + target_data) - vocab\n",
    "if missing_chars:\n",
    "    print(\"Missing characters:\", missing_chars)\n",
    "else:\n",
    "    print(\"Vocabulary covers all characters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_carriage_returns(file_path, output_path):\n",
    "    with open(file_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line.replace(\"\\r\", \"\"))\n",
    "\n",
    "\n",
    "# Clean vocabulary and data files\n",
    "remove_carriage_returns(\n",
    "    \"data/trial3/vocabulary.txt\", \"data/trial3/vocabulary_clean.txt\"\n",
    ")\n",
    "remove_carriage_returns(\n",
    "    \"data/trial3/train_inputs.txt\", \"data/trial3/train_inputs_clean.txt\"\n",
    ")\n",
    "remove_carriage_returns(\n",
    "    \"data/trial3/train_targets.txt\", \"data/trial3/train_targets_clean.txt\"\n",
    ")\n",
    "remove_carriage_returns(\n",
    "    \"data/trial3/valid_inputs.txt\", \"data/trial3/valid_inputs_clean.txt\"\n",
    ")\n",
    "remove_carriage_returns(\n",
    "    \"data/trial3/valid_targets.txt\", \"data/trial3/valid_targets_clean.txt\"\n",
    ")\n",
    "remove_carriage_returns(\n",
    "    \"data/trial3/test_inputs.txt\", \"data/trial3/test_inputs_clean.txt\"\n",
    ")\n",
    "remove_carriage_returns(\n",
    "    \"data/trial3/test_targets.txt\", \"data/trial3/test_targets_clean.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tokens: ['a\\n', 'i\\n', 'n\\n', 'r\\n', 't\\n']\n",
      "Length of first token: 1\n"
     ]
    }
   ],
   "source": [
    "with open(\"vocabulary.vocab.src.clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(f\"First 5 tokens: {lines[:5]}\")\n",
    "    print(f\"Length of first token: {len(lines[0].strip())}\")  # Should NOT include \\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenize(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            # Split into characters and join with spaces\n",
    "            char_line = \" \".join(line.strip())\n",
    "            outfile.write(char_line + \"\\n\")\n",
    "\n",
    "# Apply to your files\n",
    "char_tokenize(\"train_inputs.txt\", \"train_inputs_char.txt\")\n",
    "char_tokenize(\"train_targets.txt\", \"train_targets_char.txt\")\n",
    "char_tokenize(\"valid_inputs.txt\", \"valid_inputs_char.txt\")\n",
    "char_tokenize(\"valid_targets.txt\", \"valid_targets_char.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Line: 'a a l l a a v'\n"
     ]
    }
   ],
   "source": [
    "with open(\"..\\single_test_input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        print(f\"Input Line: {repr(line.strip())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All characters are covered in the vocabulary!\n"
     ]
    }
   ],
   "source": [
    "def check_vocab_coverage(input_file, vocab_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(vocab_file, 'r', encoding='utf-8') as vocab:\n",
    "        input_chars = set(\"\".join(infile.read().split()))\n",
    "        vocab_chars = set(line.strip() for line in vocab)\n",
    "        missing_chars = input_chars - vocab_chars\n",
    "        if missing_chars:\n",
    "            print(f\"Missing characters: {missing_chars}\")\n",
    "        else:\n",
    "            print(\"All characters are covered in the vocabulary!\")\n",
    "\n",
    "check_vocab_coverage(\"..\\single_test_input.txt\", \"vocabulary.vocab.src.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All characters in the training input are covered in the vocabulary!\n"
     ]
    }
   ],
   "source": [
    "def check_vocab_coverage(input_file, vocab_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(vocab_file, \"r\", encoding=\"utf-8\") as vocab:\n",
    "        input_chars = set(\"\".join(infile.read().split()))\n",
    "        vocab_chars = set(line.strip() for line in vocab)\n",
    "        missing_chars = input_chars - vocab_chars\n",
    "        if missing_chars:\n",
    "            print(f\"Missing characters in vocab: {missing_chars}\")\n",
    "        else:\n",
    "            print(\"All characters in the training input are covered in the vocabulary!\")\n",
    "\n",
    "check_vocab_coverage(\"train_inputs_char.txt\", \"vocabulary.vocab.src.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1430: 't t a a m m i i l l u'\n"
     ]
    }
   ],
   "source": [
    "with open(\"train_inputs_char.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 1429:  # Replace 0 with the line number you want to check\n",
    "            print(f\"Line {i+1}: {repr(line.strip())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vocab(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Clean source and target vocabulary files\n",
    "clean_vocab(\"vocabulary.vocab.src.txt\", \"vocabulary.vocab.src.clean.txt\")\n",
    "clean_vocab(\"vocabulary.vocab.tgt.txt\", \"vocabulary.vocab.tgt.clean.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.vocab.src.clean.txt\", \"rb\") as infile, open(\"vocabulary.vocab.src.fixed.txt\", \"wb\") as outfile:\n",
    "    for line in infile:\n",
    "        outfile.write(line.replace(b'\\r\\n', b'\\n').replace(b'\\r', b'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
